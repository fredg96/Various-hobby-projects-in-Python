{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs #For inspecting html webpage in notebook\n",
    "import pandas as pd #To put data into frames for joining into a final result, also sued for printing to csv\n",
    "import lxml #For parsing html\n",
    "import requests #For requesting the webpages which we will srape\n",
    "import time #To have a wait timer when scraping, for  politeness sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this project we will start with the wikipedia page detailing the current (2020-06-21) list of US congress members.\n",
    "#From this base page we can get the representative from each congress district together with data about their party affiliation\n",
    "#previous experience, education, when they assumed their current office, residence, and which year they were born.\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_current_members_of_the_United_States_House_of_Representatives\" #Url to wikipedia page\n",
    "response = requests.get(url) #The received page when requesting the specified url\n",
    "soup = bs(response.content, 'lxml') #creating a BeautifulSoup object which we can display in the notebook and inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()) #Print the parsed html page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup.find_all('table') #Returns all tables on the webpage\n",
    "tables #Print all tables in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the tables where you can sort the data on the webpage.\n",
    "members_table = soup.find_all(\"table\", class_ =\"wikitable sortable\")[2] #The webpage which we are interested in\n",
    "print(members_table.prettify()) #Print the table of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas has built in function to instantly scrape the wikipedia table and put the information into a pandas frame.\n",
    "congress_members_frame = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_current_members_of_the_United_States_House_of_Representatives\")[6] #Index specifies which table to put into a fram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_members_frame #Print the created data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will go through the table of congress members and scrape the links to their wikipedia pages\n",
    "links_to_members = [] #list to store links\n",
    "for row in members_table.findAll('tr'): #find all rows\n",
    "    cells=row.findAll('td') #find all columns\n",
    "    if len(cells)==9: #the number of columns in the table of interest is 9\n",
    "        links = cells[1].findAll('a') #By inspecting the parsed html side we can see that links are started with an a hence we want to find all links in the second column\n",
    "        if links != []: #Make sure that there is a link, vacancies have no links for example \n",
    "            link = links[1].get('href') #Since the table has a link to an image of the congress member before the link to their page we need to chose the second link\n",
    "            links_to_members.append('https://en.wikipedia.org' + link) #Add the unique link to the list  \n",
    "        else: \n",
    "            continue #If no link is found continue to next row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the list created above to visit each members page and extract the name of their spouse, if any, and number of childre, if any.\n",
    "#Names are scraped to get a unique key for later joining.\n",
    "names = [] #List to keep the names used as keys.\n",
    "spouses = [] #List to keep name of spouses\n",
    "childrens = [] #List to keep number of childrens\n",
    "for member in range(len(links_to_members)):\n",
    "    #Set the three items of interest to a base case, in case we don't find the data we want we don't want to save the data from the previous\n",
    "    #candidata again.\n",
    "    cname = \" \"\n",
    "    bname = \" \"\n",
    "    spouse = \"none\"\n",
    "    children = \" \"\n",
    "    url = links_to_members[member] #link to specific member\n",
    "    resp = requests.get(url, params={'action': 'raw'}) #request the page as raw wikidata page for easy of scrapeing the info box\n",
    "    page = resp.text\n",
    "    for line in page.splitlines(): #go through each line\n",
    "        #We are looking for names which might most likely be under birth_name, name, or Name with either a white space after the '|' or no whitespace. \n",
    "        if line.startswith('| birth_name'):\n",
    "            bname = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('|birth_name'):\n",
    "            bname = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('|name'):\n",
    "            cname = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('| name'):\n",
    "            cname = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('|Name'):\n",
    "            cname = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('| Name'):\n",
    "            cname = line.partition('=')[-1].strip()\n",
    "        #Spouse are most likelt found under spouse or Spouse\n",
    "        elif line.startswith('|spouse'):\n",
    "            spouse = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('|Spouse'):\n",
    "            spouse = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('| Spouse'):\n",
    "            spouse = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('| spouse'):\n",
    "            spouse = line.partition('=')[-1].strip()\n",
    "        #number of childrens might be udner children, Children, childrens, or Childrens\n",
    "        elif line.startswith('| children'):\n",
    "            children = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('| Children'):\n",
    "            children = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('|children'):\n",
    "            children = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('|Children'):\n",
    "            children = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('|Childrens'):\n",
    "            children = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('| Childrens'):\n",
    "            children = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('| childrens'):\n",
    "            children = line.partition('=')[-1].strip()\n",
    "        elif line.startswith('| childrens'):\n",
    "            children = line.partition('=')[-1].strip()\n",
    "        #Website appears to be the last part of the infobox so when we reach it we stop scan their page.\n",
    "        elif line.startswith('|website'):  \n",
    "            break \n",
    "        elif line.startswith('| website'):  \n",
    "            break\n",
    "    if cname != \" \": #We will prefere their called name which should correspond better between tables\n",
    "        name = cname\n",
    "    elif bname != \" \": #If we only find their birth name we will use that instead to make manual pairing easier when cleaning data\n",
    "        name = bname \n",
    "    else: #If we do not find any name we wil lfill it in as blank\n",
    "        name = \" \"\n",
    "    names.append(name) #Add the name to the list\n",
    "    spouses.append(spouse) #Add the name of the spouse to the list\n",
    "    childrens.append(children) #Add the number of childrens to the list\n",
    "    time.sleep(0.5) #Wait this time to be polite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_personal_data = pd.DataFrame(names,columns=['Member'])  #Put the new data into a frame with first column being member.\n",
    "member_personal_data['Spouse'] = spouses\n",
    "member_personal_data['Childrens'] = childrens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the two tables using the member name as the key. In this case a full outer join will be used\n",
    "#in order to include data which we fail to find the correct keys, e.g. one of frame might have th name Joe while another has the name Joseph.\n",
    "#Another alternative would be do join on the position in the frames however the vacancies will mess up this ordering so we would need to place these last, or first.\n",
    "result = pd.merge(congress_members_frame, member_personal_data,how='outer', on='Member')\n",
    "result.to_csv('congress_members.csv') #Print the results to a csv file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result #Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result #Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://en.wikipedia.org/wiki/Liz_Cheney\"\n",
    "resp = requests.get(url, params={'action': 'raw'}) #request the page as raw wikidata page for easy of scrapeing the info box\n",
    "page = resp.text\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
